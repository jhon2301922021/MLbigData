{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df952974-4a13-4fe9-af9a-8d101a5a6cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Enginering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adacd4aa-9aec-477a-a2b2-21563cfd7d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfb9ea7-bcc6-4adb-9dac-11bc363df465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Feature Extraction** o **Extracción de Características** es el proceso de derivar características útiles a partir de los datos originales que sean relevantes para el problema de **machine learning**. En PySpark, existen varias herramientas para extraer y generar nuevas características que aporten más valor al modelo, asegurando que la información más importante sea capturada de manera efectiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "789af06b-0719-41d8-9ad7-6ed7b7ac6268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "**TF-IDF** es una técnica en procesamiento de lenguaje natural y minería de textos que evalúa la importancia de una palabra en un documento en relación con un conjunto de documentos (corpus). Se utiliza ampliamente en tareas como la recuperación de información, el análisis de texto y la clasificación.\n",
    "\n",
    "#### Componentes de TF-IDF\n",
    "\n",
    "1. #### Term Frequency (TF) - Frecuencia de Término\n",
    "   La **Frecuencia de Término** mide cuántas veces aparece una palabra en un documento específico. Es una medida de la frecuencia relativa de una palabra dentro de un documento particular.\n",
    "\n",
    "   - **Fórmula**:\n",
    "     ```\n",
    "     TF = (Número de veces que aparece la palabra en el documento) / (Número total de palabras en el documento)\n",
    "     ```\n",
    "\n",
    "   - **Ejemplo**:\n",
    "     Si la palabra \"gato\" aparece 3 veces en un documento de 100 palabras, entonces:\n",
    "     ```\n",
    "     TF = 3 / 100 = 0.03\n",
    "     ```\n",
    "\n",
    "2. #### Inverse Document Frequency (IDF) - Frecuencia Inversa de Documento\n",
    "   La **Frecuencia Inversa de Documento** mide la importancia de una palabra dentro del corpus completo. Penaliza las palabras que aparecen en muchos documentos, dándoles menor peso, mientras que aumenta el peso de palabras que aparecen en pocos documentos.\n",
    "\n",
    "   - **Fórmula**:\n",
    "     ```\n",
    "     IDF = log(Número total de documentos / Número de documentos que contienen la palabra)\n",
    "     ```\n",
    "\n",
    "   - **Ejemplo**:\n",
    "     Si tenemos un corpus de 1000 documentos, y la palabra \"gato\" aparece en 10 de ellos, el IDF es:\n",
    "     ```\n",
    "     IDF = log(1000 / 10) = log(100) ≈ 2\n",
    "     ```\n",
    "\n",
    "3. #### TF-IDF\n",
    "   La medida **TF-IDF** combina la frecuencia de término y la frecuencia inversa de documento para cada palabra en un documento específico. Multiplica **TF** e **IDF** para reflejar la relevancia de una palabra en un documento respecto al corpus.\n",
    "\n",
    "   - **Fórmula**:\n",
    "     ```\n",
    "     TF-IDF = TF * IDF\n",
    "     ```\n",
    "\n",
    "   - **Interpretación**: \n",
    "     Un valor alto de TF-IDF indica que la palabra es relevante para ese documento en particular y poco común en otros documentos. Por el contrario, palabras comunes en el corpus tienen valores bajos de TF-IDF, ya que aportan poca relevancia específica.\n",
    "\n",
    "\n",
    "#### Ejemplo de Cálculo de TF-IDF\n",
    "\n",
    "Supongamos un corpus con 3 documentos:\n",
    "- **Documento 1**: \"El gato duerme en el sofá.\"\n",
    "- **Documento 2**: \"El perro duerme en la alfombra.\"\n",
    "- **Documento 3**: \"El gato y el perro juegan en el jardín.\"\n",
    "\n",
    "Deseamos calcular el valor TF-IDF de la palabra \"gato\" en el Documento 1.\n",
    "\n",
    "1. **Cálculo de TF** para \"gato\" en Documento 1:\n",
    "   - \"Gato\" aparece una vez en un documento de 5 palabras.\n",
    "   - ```\n",
    "     TF = 1 / 5 = 0.2\n",
    "     ```\n",
    "\n",
    "2. **Cálculo de IDF** para \"gato\":\n",
    "   - La palabra \"gato\" aparece en 2 de los 3 documentos.\n",
    "   - ```\n",
    "     IDF = log(3 / 2) ≈ 0.176\n",
    "     ```\n",
    "\n",
    "3. **Cálculo de TF-IDF**:\n",
    "   - ```\n",
    "     TF-IDF = 0.2 * 0.176 ≈ 0.0352\n",
    "     ```\n",
    "\n",
    "#### Aplicaciones de TF-IDF\n",
    "\n",
    "- **Búsqueda de Información**: Ayuda a priorizar documentos que contienen términos relevantes para una consulta específica.\n",
    "- **Clasificación de Textos**: Transforma el texto en vectores de características numéricas que pueden usarse en algoritmos de clasificación.\n",
    "- **Análisis de Temas y Palabras Clave**: Permite identificar palabras clave representativas en documentos o temas específicos dentro de un corpus.\n",
    "\n",
    "TF-IDF es ampliamente usado en tareas de análisis de texto y modelado en bibliotecas como **scikit-learn** y **PySpark**, donde el cálculo se realiza automáticamente usando herramientas como `TfidfVectorizer` o `HashingTF` y `IDF`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba721a1-3c0c-4929-a7b6-5ff5a216bba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|document|sentence                |\n",
      "+--------+------------------------+\n",
      "|0       |Fútbol baloncesto tenis |\n",
      "|1       |Fútbol tecnología IA    |\n",
      "|2       |Tenis baloncesto        |\n",
      "|3       |Tecnología innovación IA|\n",
      "|4       |Fútbol deportes         |\n",
      "+--------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "sentenceData = spark.createDataFrame([(0, \"Fútbol baloncesto tenis\"),\n",
    "                                      (1, \"Fútbol tecnología IA\"),\n",
    "                                      (2, \"Tenis baloncesto\"),\n",
    "                                      (3, \"Tecnología innovación IA\"),\n",
    "                                      (4, \"Fútbol deportes\")\n",
    "                                      ], [\"document\", \"sentence\"])\n",
    "sentenceData.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d63e582-dab6-4109-94fa-4be9a7ec47a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### CountVectorizer\n",
    "\n",
    "**CountVectorizer** es una técnica en procesamiento de lenguaje natural que convierte texto en una representación numérica basada en la frecuencia de palabras. Cada documento se transforma en un vector donde cada elemento representa la frecuencia de una palabra específica en ese documento.\n",
    "\n",
    "##### Funcionamiento Básico\n",
    "\n",
    "- `CountVectorizer` toma una colección de documentos y genera una **matriz de frecuencias de términos**.\n",
    "- Cada fila representa un documento, y cada columna representa una palabra única en el corpus.\n",
    "- Los valores en la matriz indican el número de veces que cada término aparece en cada documento.\n",
    "\n",
    "##### Parámetros Clave\n",
    "\n",
    "- **`vocabSize`**: Número máximo de términos únicos que se incluirán.\n",
    "- **`minDF`**: Mínima frecuencia de documentos en los que debe aparecer un término para ser incluido.\n",
    "- **`binary`**: Si es `True`, indica solo la presencia o ausencia de términos en lugar de la frecuencia.\n",
    "\n",
    "##### Ejemplo\n",
    "\n",
    "Para un corpus con dos documentos:\n",
    "- Documento 1: \"El perro juega en el parque\"\n",
    "- Documento 2: \"El gato duerme en el sofá\"\n",
    "\n",
    "La matriz resultante podría verse así:\n",
    "\n",
    "| Documento   | el | perro | juega | en | parque | gato | duerme | sofá |\n",
    "|-------------|----|-------|-------|----|--------|------|--------|------|\n",
    "| Documento 1 | 2  | 1     | 1     | 1  | 1      | 0    | 0      | 0    |\n",
    "| Documento 2 | 2  | 0     | 0     | 1  | 0      | 1    | 1      | 1    |\n",
    "\n",
    "`CountVectorizer` es útil para modelos de machine learning que requieren datos estructurados. En PySpark, se usa mediante la clase `pyspark.ml.feature.CountVectorizer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3f5394-7d5f-43b7-9a7f-3d7764b0654b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado de TF-IDF con el nuevo dataset:\n",
      "+--------+------------------------+----------------------------+-------------------------+----------------------------------------------------------------------+\n",
      "|document|sentence                |words                       |rawFeatures              |features                                                              |\n",
      "+--------+------------------------+----------------------------+-------------------------+----------------------------------------------------------------------+\n",
      "|0       |Fútbol baloncesto tenis |[fútbol, baloncesto, tenis] |(7,[0,1,2],[1.0,1.0,1.0])|(7,[0,1,2],[0.4054651081081644,0.6931471805599453,0.6931471805599453])|\n",
      "|1       |Fútbol tecnología IA    |[fútbol, tecnología, ia]    |(7,[0,3,4],[1.0,1.0,1.0])|(7,[0,3,4],[0.4054651081081644,0.6931471805599453,0.6931471805599453])|\n",
      "|2       |Tenis baloncesto        |[tenis, baloncesto]         |(7,[1,2],[1.0,1.0])      |(7,[1,2],[0.6931471805599453,0.6931471805599453])                     |\n",
      "|3       |Tecnología innovación IA|[tecnología, innovación, ia]|(7,[3,4,5],[1.0,1.0,1.0])|(7,[3,4,5],[0.6931471805599453,0.6931471805599453,1.0986122886681098])|\n",
      "|4       |Fútbol deportes         |[fútbol, deportes]          |(7,[0,6],[1.0,1.0])      |(7,[0,6],[0.4054651081081644,1.0986122886681098])                     |\n",
      "+--------+------------------------+----------------------------+-------------------------+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer: Divide la columna 'sentence' en palabras\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "# CountVectorizer: Convierte las palabras en una matriz de frecuencia de términos (TF)\n",
    "vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "\n",
    "# IDF: Calcula la frecuencia inversa de documento para los términos\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Pipeline: Agrupa todas las etapas en un flujo de trabajo\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "# Entrena el modelo de Pipeline\n",
    "model = pipeline.fit(sentenceData)\n",
    "\n",
    "# Transforma los datos de entrada para ver los resultados de TF-IDF\n",
    "result = model.transform(sentenceData)\n",
    "print(\"Resultado de TF-IDF con el nuevo dataset:\")\n",
    "result.select(\"document\", \"sentence\", \"words\", \"rawFeatures\", \"features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b9b0f00-816c-47dc-9b7f-302e06475436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario y conteo total de términos:\n",
      "+----------+------+\n",
      "| vocabList|counts|\n",
      "+----------+------+\n",
      "|    fútbol|   3.0|\n",
      "|baloncesto|   2.0|\n",
      "|     tenis|   2.0|\n",
      "|tecnología|   2.0|\n",
      "|        ia|   2.0|\n",
      "|innovación|   1.0|\n",
      "|  deportes|   1.0|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calcular el conteo total de cada término en el corpus\n",
    "total_counts = model.transform(sentenceData) \\\n",
    "    .select('rawFeatures').rdd \\\n",
    "    .map(lambda row: row['rawFeatures'].toArray()) \\\n",
    "    .reduce(lambda x, y: [x[i] + y[i] for i in range(len(y))])\n",
    "\n",
    "# Obtener el vocabulario de CountVectorizer\n",
    "vocabList = model.stages[1].vocabulary\n",
    "d = {'vocabList': vocabList, 'counts': total_counts}\n",
    "\n",
    "# Mostrar el vocabulario y sus frecuencias en el corpus\n",
    "print(\"Vocabulario y conteo total de términos:\")\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(), list(d.keys())).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2970e5-1ee1-4aee-88b1-592fbeecac6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteos de términos en cada documento:\n",
      "[Row(rawFeatures=SparseVector(7, {0: 1.0, 1: 1.0, 2: 1.0})), Row(rawFeatures=SparseVector(7, {0: 1.0, 3: 1.0, 4: 1.0})), Row(rawFeatures=SparseVector(7, {1: 1.0, 2: 1.0})), Row(rawFeatures=SparseVector(7, {3: 1.0, 4: 1.0, 5: 1.0})), Row(rawFeatures=SparseVector(7, {0: 1.0, 6: 1.0}))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mostrar el contenido de rawFeatures\n",
    "counts = model.transform(sentenceData).select('rawFeatures').collect()\n",
    "print(\"Conteos de términos en cada documento:\")\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed695b2-2334-49f3-b8c6-c6c30bf6c086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw features con índices, valores y términos correspondientes:\n",
      "+-------------------------+---------+-----------------------------------+----------------------------+\n",
      "|rawFeatures              |indices  |values                             |Terms                       |\n",
      "+-------------------------+---------+-----------------------------------+----------------------------+\n",
      "|(7,[0,1,2],[1.0,1.0,1.0])|[0, 1, 2]|[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]|[fútbol, baloncesto, tenis] |\n",
      "|(7,[0,3,4],[1.0,1.0,1.0])|[0, 3, 4]|[1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]|[fútbol, tecnología, ia]    |\n",
      "|(7,[1,2],[1.0,1.0])      |[1, 2]   |[0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]|[baloncesto, tenis]         |\n",
      "|(7,[3,4,5],[1.0,1.0,1.0])|[3, 4, 5]|[0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]|[tecnología, ia, innovación]|\n",
      "|(7,[0,6],[1.0,1.0])      |[0, 6]   |[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]|[fútbol, deportes]          |\n",
      "+-------------------------+---------+-----------------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Función para convertir índices de términos en términos reales\n",
    "def termsIdx2Term(vocabulary):\n",
    "    def termsIdx2Term(termIndices):\n",
    "        return [vocabulary[int(index)] for index in termIndices]\n",
    "    return udf(termsIdx2Term, ArrayType(StringType()))\n",
    "\n",
    "# Aplicar la función para convertir rawFeatures a términos legibles\n",
    "vectorizerModel = model.stages[1]\n",
    "vocabList = vectorizerModel.vocabulary\n",
    "rawFeatures = model.transform(sentenceData).select('rawFeatures')\n",
    "\n",
    "# Mostrar el DataFrame con índices, valores y términos\n",
    "indices_udf = udf(lambda vector: vector.indices.tolist(), ArrayType(IntegerType()))\n",
    "values_udf = udf(lambda vector: vector.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "print(\"Raw features con índices, valores y términos correspondientes:\")\n",
    "rawFeatures.withColumn('indices', indices_udf(F.col('rawFeatures'))) \\\n",
    "           .withColumn('values', values_udf(F.col('rawFeatures'))) \\\n",
    "           .withColumn(\"Terms\", termsIdx2Term(vocabList)(\"indices\")) \\\n",
    "           .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f784f2c7-aedb-4971-a6b4-d67500771517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### HashingTF\n",
    "\n",
    "**HashingTF** es otra técnica para convertir texto en una representación numérica, pero en lugar de construir un vocabulario explícito, usa una función de **hashing** para mapear palabras a posiciones en un vector de tamaño fijo.\n",
    "\n",
    "###### Funcionamiento Básico de HashingTF\n",
    "- **Hashing**: En `HashingTF`, cada palabra se convierte en un índice en el vector mediante una función de hash, lo que permite definir el tamaño del vector (`numFeatures`) de antemano.\n",
    "- **Colisiones de Hash**: Dado que usa hashing, diferentes palabras pueden mapearse a la misma posición, lo que genera colisiones. Esto puede reducir la precisión en algunos casos, pero mejora la eficiencia y la escalabilidad.\n",
    "\n",
    "###### Diferencias entre CountVectorizer y HashingTF\n",
    "\n",
    "| Aspecto               | CountVectorizer                          | HashingTF                                  |\n",
    "|-----------------------|------------------------------------------|--------------------------------------------|\n",
    "| **Vocabulario**       | Construye un vocabulario explícito       | No construye vocabulario, usa una función de hash |\n",
    "| **Tamaño del Vector** | Depende del tamaño del vocabulario       | Fijo, determinado por `numFeatures`       |\n",
    "| **Precisión**         | Alta (sin colisiones)                    | Puede sufrir colisiones                   |\n",
    "| **Memoria**           | Puede ser intensivo en vocabularios grandes | Eficiente, adecuado para grandes conjuntos de datos |\n",
    "| **Interpretabilidad** | Fácilmente interpretable                 | Menos interpretable debido a las colisiones |\n",
    "\n",
    "##### ¿Cuándo usar cada uno?\n",
    "\n",
    "- **Usa `CountVectorizer`** cuando el vocabulario no es extremadamente grande y necesitas precisión en la representación de los términos.\n",
    "- **Usa `HashingTF`** en corpus grandes o cuando necesitas limitar el tamaño del vector para reducir el uso de memoria, aunque puede haber una ligera pérdida de precisión debido a colisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0094dc-ac5a-4795-824d-100742f15aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+------------------------------+-------------------+----------------------------------+\n",
      "|document|sentence                 |words                         |rawFeatures        |features                          |\n",
      "+--------+-------------------------+------------------------------+-------------------+----------------------------------+\n",
      "|0       |Python python Spark Spark|[python, python, spark, spark]|(5,[1,4],[2.0,2.0])|(5,[1,4],[0.8109302162163288,0.0])|\n",
      "|1       |Python SQL               |[python, sql]                 |(5,[2,4],[1.0,1.0])|(5,[2,4],[0.4054651081081644,0.0])|\n",
      "+--------+-------------------------+------------------------------+-------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"Python python Spark Spark\"),\n",
    "    (1, \"Python SQL\")\n",
    "], [\"document\", \"sentence\"])\n",
    "\n",
    "# Tokenizer: Divide la columna 'sentence' en palabras\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "# HashingTF: Convierte las palabras en una matriz de frecuencia de términos utilizando hashing\n",
    "vectorizer = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=5)\n",
    "\n",
    "# IDF: Calcula la frecuencia inversa de documentos para los términos\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Pipeline: Agrupa todas las etapas en un flujo de trabajo\n",
    "pipeline = Pipeline(stages=[tokenizer, vectorizer, idf])\n",
    "\n",
    "# Entrena el modelo de Pipeline\n",
    "model = pipeline.fit(sentenceData)\n",
    "\n",
    "# Transforma los datos de entrada para ver los resultados de TF-IDF\n",
    "result = model.transform(sentenceData)\n",
    "\n",
    "# Muestra el resultado final\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1464c257-8a3c-4fb2-8d84-0832aa56d549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d88095a0-8575-4436-a25d-f13aa074f75f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Word2Vec** es un modelo de aprendizaje profundo que convierte palabras en vectores numéricos, capturando relaciones semánticas entre ellas. En Spark, `Word2Vec` toma un conjunto de documentos tokenizados y asigna a cada palabra un vector en un espacio de características, donde palabras con significados similares tienen representaciones vectoriales cercanas.\n",
    "\n",
    "##### Funcionamiento Básico\n",
    "- `Word2Vec` aprende una representación densa para cada palabra en función de su contexto en una ventana de palabras (tamaño definido por el parámetro `windowSize`).\n",
    "- Los vectores de palabras pueden usarse en tareas de NLP como análisis de similitud o clustering.\n",
    "\n",
    "##### Parámetros Clave\n",
    "- **`vectorSize`**: Define el tamaño del vector resultante para cada palabra.\n",
    "- **`windowSize`**: Especifica el tamaño de la ventana de contexto utilizada para el entrenamiento.\n",
    "- **`minCount`**: Número mínimo de ocurrencias de una palabra para incluirla en el modelo.\n",
    "\n",
    "En PySpark, el modelo `Word2Vec` se encuentra en `pyspark.ml.feature.Word2Vec` y se entrena usando un pipeline para obtener representaciones vectoriales de palabras en textos grandes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96336dc2-2512-4785-95cd-97007c0801a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Word Embedding Models in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc01db7b-83d2-4d1a-aa34-07f49bf7e188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------+\n",
      "|document|sentence                                         |words                                                      |feature                                                          |\n",
      "+--------+-------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------+\n",
      "|0       |El fútbol es un deporte muy popular              |[el, fútbol, es, un, deporte, muy, popular]                |[-0.03954152097659452,-0.008140003042561668,-0.01849026765142168]|\n",
      "|1       |La tecnología avanza rápidamente                 |[la, tecnología, avanza, rápidamente]                      |[-0.0469239829108119,0.03239990258589387,0.026472446508705616]   |\n",
      "|2       |Los deportes y la tecnología son temas de interés|[los, deportes, y, la, tecnología, son, temas, de, interés]|[0.0347811355928166,0.0030308921510974565,-5.57354340950648E-4]  |\n",
      "+--------+-------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Ejemplo de datos: sentenceData\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"El fútbol es un deporte muy popular\"),\n",
    "    (1, \"La tecnología avanza rápidamente\"),\n",
    "    (2, \"Los deportes y la tecnología son temas de interés\")\n",
    "], [\"document\", \"sentence\"])\n",
    "\n",
    "# Tokenizer: Convierte la columna 'sentence' en una lista de palabras 'words'\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "# Word2Vec: Convierte las palabras en vectores de características\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"feature\")\n",
    "\n",
    "# Pipeline: Agrupa las etapas de Tokenizer y Word2Vec en un flujo de trabajo\n",
    "pipeline = Pipeline(stages=[tokenizer, word2Vec])\n",
    "\n",
    "# Entrena el modelo de Word2Vec\n",
    "model = pipeline.fit(sentenceData)\n",
    "\n",
    "# Transforma los datos de entrada para obtener los vectores de características\n",
    "result = model.transform(sentenceData)\n",
    "\n",
    "# Muestra el resultado final\n",
    "result.select(\"document\", \"sentence\", \"words\", \"feature\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f17f28a-1ed3-40fb-a126-402becb22222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectores de palabras individuales:\n",
      "+-----------+------------------------------------------------------------------+\n",
      "|word       |vector                                                            |\n",
      "+-----------+------------------------------------------------------------------+\n",
      "|muy        |[-0.15639916062355042,-0.14967060089111328,-0.12522591650485992]  |\n",
      "|tecnología |[-0.03242826089262962,0.025772446766495705,0.03958774730563164]   |\n",
      "|y          |[-0.02910229191184044,0.14060592651367188,0.010431595146656036]   |\n",
      "|interés    |[0.0013236792292445898,0.010991252027451992,-0.11285977810621262] |\n",
      "|temas      |[0.09195507317781448,0.14056122303009033,-0.008430935442447662]   |\n",
      "|fútbol     |[-0.08341887593269348,0.0737525001168251,0.12147324532270432]     |\n",
      "|el         |[-0.13390909135341644,0.03708652779459953,0.07401996105909348]    |\n",
      "|la         |[-0.09924070537090302,0.015769578516483307,0.03510101139545441]   |\n",
      "|deportes   |[0.12654274702072144,0.06648338586091995,0.13824526965618134]     |\n",
      "|popular    |[-0.02327553741633892,-0.036884453147649765,-0.06552619487047195] |\n",
      "|de         |[0.12052521109580994,-0.07061354070901871,-0.16544730961322784]   |\n",
      "|los        |[0.15257954597473145,-0.13740934431552887,0.07682155817747116]    |\n",
      "|son        |[-0.019124777987599373,-0.16488289833068848,-0.018465347588062286]|\n",
      "|deporte    |[0.10575635731220245,-0.09007421880960464,-0.10482945293188095]   |\n",
      "|avanza     |[-0.15398958325386047,0.1327204555273056,-0.08621825277805328]    |\n",
      "|un         |[-0.10483893007040024,0.16575686633586884,-0.08379663527011871]   |\n",
      "|es         |[0.11929459124803543,-0.05694664269685745,0.05445311963558197]    |\n",
      "|rápidamente|[0.09796261787414551,-0.04466287046670914,0.1174192801117897]     |\n",
      "+-----------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtener los vectores de cada palabra en el modelo Word2Vec\n",
    "w2v = model.stages[1]  # Accede a la etapa de Word2Vec en el pipeline\n",
    "print(\"Vectores de palabras individuales:\")\n",
    "w2v.getVectors().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19716e1-e7ba-4614-84fe-93e2630f91ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. FeatureHasher \n",
    "\n",
    "**FeatureHasher** es una técnica en Spark para convertir varias columnas de diferentes tipos (numéricas, categóricas y booleanas) en un único vector de características, utilizando un método de **hashing**. Esto es útil para transformar datos de alto volumen y reducir el espacio de características, especialmente en conjuntos de datos que incluyen columnas categóricas de gran tamaño.\n",
    "\n",
    "#### Funcionamiento Básico\n",
    "\n",
    "- **Hashing**: Cada valor en las columnas de entrada se convierte en una posición en el vector de salida usando una función de hash. Esto permite representar grandes volúmenes de datos categóricos en un espacio vectorial de tamaño fijo.\n",
    "- **Vector de Características**: `FeatureHasher` produce un vector disperso de tamaño fijo en la columna de salida especificada. Las entradas numéricas se copian directamente al vector, mientras que las entradas categóricas y booleanas se convierten en índices de características mediante hashing.\n",
    "\n",
    "#### Parámetros Clave\n",
    "\n",
    "- **`inputCols`**: Lista de las columnas de entrada que se van a transformar.\n",
    "- **`outputCol`**: Nombre de la columna de salida que contendrá el vector de características.\n",
    "- **`numFeatures`** (opcional): Número de características en el vector de salida. Si no se especifica, Spark utiliza un valor predeterminado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8d2fa97-bbb2-494f-b18c-be9bb09841d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------------+-----------+--------------------------------------------------------+\n",
      "|price|in_stock|product_code|category   |features                                                |\n",
      "+-----+--------+------------+-----------+--------------------------------------------------------+\n",
      "|12.5 |true    |101         |electronics|(262144,[16758,162583,201386,211061],[1.0,1.0,12.5,1.0])|\n",
      "|8.3  |false   |102         |clothing   |(262144,[61126,123535,183867,201386],[1.0,1.0,1.0,8.3]) |\n",
      "|15.7 |true    |103         |furniture  |(262144,[59816,148779,162583,201386],[1.0,1.0,1.0,15.7])|\n",
      "|3.2  |false   |104         |electronics|(262144,[183867,199043,201386,211061],[1.0,1.0,3.2,1.0])|\n",
      "|7.5  |true    |105         |groceries  |(262144,[23793,162583,201386,259376],[1.0,1.0,7.5,1.0]) |\n",
      "|6.0  |false   |106         |clothing   |(262144,[21292,61126,183867,201386],[1.0,1.0,1.0,6.0])  |\n",
      "+-----+--------+------------+-----------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "# Nuevo dataset de ejemplo sobre productos\n",
    "dataset = spark.createDataFrame([\n",
    "    (12.5, True, \"101\", \"electronics\"),\n",
    "    (8.3, False, \"102\", \"clothing\"),\n",
    "    (15.7, True, \"103\", \"furniture\"),\n",
    "    (3.2, False, \"104\", \"electronics\"),\n",
    "    (7.5, True, \"105\", \"groceries\"),\n",
    "    (6.0, False, \"106\", \"clothing\")\n",
    "], [\"price\", \"in_stock\", \"product_code\", \"category\"])\n",
    "\n",
    "# Definir el FeatureHasher\n",
    "hasher = FeatureHasher(inputCols=[\"price\", \"in_stock\", \"product_code\", \"category\"], outputCol=\"features\")\n",
    "\n",
    "# Transformar el DataFrame\n",
    "featurized = hasher.transform(dataset)\n",
    "\n",
    "# Mostrar el resultado\n",
    "featurized.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83603a4b-5d7c-464c-8b30-b6e43c0e71d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. RFormula\n",
    "\n",
    "**RFormula** es una herramienta en PySpark para transformar datos en un formato adecuado para machine learning, inspirada en la sintaxis de fórmulas de **R**. `RFormula` permite especificar relaciones entre características (features) y etiquetas (label) de una manera concisa, y es útil para crear automáticamente vectores de características y convertir variables categóricas en variables numéricas.\n",
    "\n",
    "#### Funcionamiento Básico\n",
    "\n",
    "- **Fórmula**: La fórmula de `RFormula` sigue el estilo de R y tiene la forma `label ~ feature1 + feature2 + ...`. Aquí, `label` es la variable objetivo (dependiente) y `feature1`, `feature2`, etc., son las variables de entrada (independientes).\n",
    "- **Codificación de Variables Categóricas**: `RFormula` convierte automáticamente las columnas categóricas en representaciones numéricas usando codificación de una sola categoría (one-hot encoding), si es necesario.\n",
    "- **Vector de Características**: Las características se agrupan en un único vector llamado `features`, y la columna de salida para la etiqueta se llama `label`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26981c3d-ce86-4e3c-8470-80d649f52fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+----------+------+\n",
      "|id |gender|age|income|features  |label |\n",
      "+---+------+---+------+----------+------+\n",
      "|1  |M     |25 |3000.0|[0.0,25.0]|3000.0|\n",
      "|2  |F     |30 |4000.0|[1.0,30.0]|4000.0|\n",
      "|3  |M     |35 |5000.0|[0.0,35.0]|5000.0|\n",
      "|4  |F     |40 |6000.0|[1.0,40.0]|6000.0|\n",
      "+---+------+---+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "# Crear un DataFrame de ejemplo\n",
    "dataset = spark.createDataFrame([\n",
    "    (1, \"M\", 25, 3000.0),\n",
    "    (2, \"F\", 30, 4000.0),\n",
    "    (3, \"M\", 35, 5000.0),\n",
    "    (4, \"F\", 40, 6000.0)\n",
    "], [\"id\", \"gender\", \"age\", \"income\"])\n",
    "\n",
    "# Definir la fórmula: \"income\" es la etiqueta y \"gender\" y \"age\" son características\n",
    "formula = RFormula(formula=\"income ~ gender + age\", featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Transformar el DataFrame\n",
    "output = formula.fit(dataset).transform(dataset)\n",
    "\n",
    "# Mostrar el resultado\n",
    "output.select(\"id\", \"gender\", \"age\", \"income\", \"features\", \"label\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e099ab2b-7d54-4322-aa1f-3bc363df8361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2078ac20-feee-4e3b-b004-6e106b2f876e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Feature Transformers** son técnicas en **PySpark** utilizadas para **transformar las características** (features) de los datos, generalmente como parte del preprocesamiento antes de aplicar algoritmos de machine learning. Estos transformadores permiten ajustar, modificar y mejorar los datos para que los algoritmos de aprendizaje automático puedan procesarlos de manera más eficiente y precisa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69cb788e-2515-49df-9eab-56388b6aebfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Tokenizer\n",
    "\n",
    "**Tokenizer** es una técnica en procesamiento de lenguaje natural (NLP) que consiste en dividir un texto en unidades más pequeñas llamadas *tokens*. Estos *tokens* suelen ser palabras individuales, pero también pueden ser frases o caracteres. El proceso de tokenización convierte una secuencia de texto en una secuencia de *tokens*, que luego puede analizarse matemáticamente en tareas de NLP.\n",
    "\n",
    "#### Tipos de Tokenizadores en PySpark\n",
    "\n",
    "1. **Tokenizer**:\n",
    "   - **Función**: Divide el texto en palabras, usando espacios en blanco como delimitador predeterminado. Cada palabra se convierte en un token.\n",
    "   - **Aplicación**: Es ideal para textos donde las palabras están separadas por espacios y no se requiere procesar caracteres especiales o puntuación.\n",
    "   - **Ejemplo**:\n",
    "     - Entrada: `\"Hi I heard about Spark\"`\n",
    "     - Salida: `[\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]`\n",
    "\n",
    "2. **RegexTokenizer**:\n",
    "   - **Función**: Permite un control más avanzado en la tokenización, usando expresiones regulares para definir cómo separar los tokens.\n",
    "   - **Aplicación**: Se utiliza cuando se necesita una tokenización más compleja, por ejemplo, para dividir por caracteres especiales, eliminar puntuación, o controlar el tratamiento de espacios.\n",
    "   - **Patrón Común**: `pattern=\"\\\\W\"` separa el texto en palabras ignorando todos los caracteres que no sean alfanuméricos, eliminando signos de puntuación.\n",
    "   - **Ejemplo**:\n",
    "     - Entrada: `\"Logistic,regression,models,are,neat\"`\n",
    "     - Salida con `pattern=\"\\\\W\"`: `[\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbec9f6f-c17c-4fe0-bcc5-e8811b74bf49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+----------------------------------------+------+\n",
      "|sentence                         |words                                   |tokens|\n",
      "+---------------------------------+----------------------------------------+------+\n",
      "|La ciencia de datos es increíble!|[la, ciencia, de, datos, es, increíble!]|6     |\n",
      "|Machine-learning, IA y datos     |[machine-learning,, ia, y, datos]       |4     |\n",
      "|Análisis de Big-data es esencial.|[análisis, de, big-data, es, esencial.] |5     |\n",
      "+---------------------------------+----------------------------------------+------+\n",
      "\n",
      "+---------------------------------+----------------------------------------+------+\n",
      "|sentence                         |words                                   |tokens|\n",
      "+---------------------------------+----------------------------------------+------+\n",
      "|La ciencia de datos es increíble!|[la, ciencia, de, datos, es, incre, ble]|7     |\n",
      "|Machine-learning, IA y datos     |[machine, learning, ia, y, datos]       |5     |\n",
      "|Análisis de Big-data es esencial.|[an, lisis, de, big, data, es, esencial]|7     |\n",
      "+---------------------------------+----------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Nuevo DataFrame de ejemplo\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"La ciencia de datos es increíble!\"),\n",
    "    (1, \"Machine-learning, IA y datos\"),\n",
    "    (2, \"Análisis de Big-data es esencial.\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "# Definir Tokenizer (basado en espacios)\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "# Definir RegexTokenizer (basado en expresiones regulares para ignorar caracteres no alfanuméricos)\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# UDF para contar el número de tokens\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "# Tokenizar y contar tokens con Tokenizer\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "         .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "# Tokenizar y contar tokens con RegexTokenizer\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\")\\\n",
    "              .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c812dec5-947f-4689-9319-d55efc965cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. StopWordsRemover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f15eb8b5-9823-495d-97a3-5ddee7640056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**StopWordsRemover** es una herramienta en PySpark que elimina *stop words* de una columna de texto tokenizado. Las *stop words* son palabras comunes en un idioma (como \"y\", \"el\", \"de\" en español o \"and\", \"the\", \"of\" en inglés) que tienen poco valor informativo y suelen eliminarse en el procesamiento de texto para reducir el ruido y mejorar la calidad del análisis.\n",
    "\n",
    "#### ¿Cómo Funciona?\n",
    "\n",
    "1. **Entrada de Texto Tokenizado**:\n",
    "   - `StopWordsRemover` requiere una columna de texto ya tokenizado, es decir, una columna donde el texto esté dividido en palabras individuales o *tokens*. Esto significa que normalmente se aplica después de una etapa de tokenización, como `Tokenizer` o `RegexTokenizer`.\n",
    "\n",
    "2. **Eliminación de Stop Words**:\n",
    "   - El transformador elimina los tokens que coinciden con palabras de una lista de *stop words* predefinida en PySpark para un idioma específico.\n",
    "   - PySpark incluye listas de *stop words* en varios idiomas, y también permite definir una lista personalizada.\n",
    "\n",
    "3. **Salida**:\n",
    "   - El `StopWordsRemover` genera una nueva columna con las *stop words* eliminadas, dejando solo las palabras informativas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7304e27d-bf6b-4656-8c24-52cbf59254dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-------------------------------------+\n",
      "|raw                                          |removed                              |\n",
      "+---------------------------------------------+-------------------------------------+\n",
      "|[me, gusta, la, naturaleza, y, los, animales]|[gusta, naturaleza, animales]        |\n",
      "|[es, importante, cuidar, el, medio, ambiente]|[importante, cuidar, medio, ambiente]|\n",
      "|[la, tecnología, avanza, muy, rápido]        |[tecnología, avanza, rápido]         |\n",
      "+---------------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# Crear un DataFrame de ejemplo con frases en español\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"me\", \"gusta\", \"la\", \"naturaleza\", \"y\", \"los\", \"animales\"]),\n",
    "    (1, [\"es\", \"importante\", \"cuidar\", \"el\", \"medio\", \"ambiente\"]),\n",
    "    (2, [\"la\", \"tecnología\", \"avanza\", \"muy\", \"rápido\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "# Configurar StopWordsRemover con stop words en español\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"removed\", stopWords=[\"me\", \"la\", \"y\", \"los\", \"es\", \"el\", \"muy\"])\n",
    "\n",
    "# Aplicar StopWordsRemover y mostrar el resultado\n",
    "cleanedData = remover.transform(sentenceData)\n",
    "cleanedData.select(\"raw\", \"removed\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "082cf5e2-80e4-4c60-b5da-bc743e9f4fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. NGram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c6b1df9-2c5a-46b6-ae92-014c61396c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**NGram** es una técnica en procesamiento de lenguaje natural (NLP) que agrupa palabras consecutivas en secuencias de longitud `n`, conocidas como *n-gramas*. Estas secuencias de palabras permiten capturar patrones y dependencias en el texto, lo cual es especialmente útil en modelos de lenguaje y en tareas de clasificación de texto. \n",
    "\n",
    "En PySpark, el transformador `NGram` toma una columna de texto tokenizado y genera una nueva columna con las secuencias de *n-gramas*.\n",
    "\n",
    "#### ¿Qué es un N-Grama?\n",
    "\n",
    "Un *n-grama* es una secuencia de `n` palabras consecutivas en una frase o documento. \n",
    "- Cuando `n=1`, se llama **unigrama** y cada token es una sola palabra.\n",
    "- Cuando `n=2`, se llama **bigrama** y cada token es una secuencia de dos palabras consecutivas.\n",
    "- Cuando `n=3`, se llama **trigrama** y cada token es una secuencia de tres palabras consecutivas.\n",
    "\n",
    "Por ejemplo, en la frase `\"El análisis de datos es importante\"`:\n",
    "- Los bigramas serían: `[\"El análisis\", \"análisis de\", \"de datos\", \"datos es\", \"es importante\"]`\n",
    "- Los trigramas serían: `[\"El análisis de\", \"análisis de datos\", \"de datos es\", \"datos es importante\"]`\n",
    "\n",
    "#### Importancia de los N-Gramas\n",
    "\n",
    "- **Contexto de Palabras**: Los *n-gramas* ayudan a capturar el contexto en el que las palabras aparecen, lo cual es crucial para modelos de lenguaje, análisis de sentimiento y clasificación de texto.\n",
    "- **Modelado de Dependencias**: Los *n-gramas* permiten modelar dependencias locales en el texto, ayudando a identificar patrones o expresiones frecuentes.\n",
    "- **Preparación de Datos para Modelos**: En el aprendizaje automático, los *n-gramas* permiten que los modelos de NLP capturen relaciones entre palabras en función de su proximidad, mejorando la precisión del análisis de texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d04ae3a-7154-40ab-aaf2-82b40e5a6943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------+-----------------------------------------------+\n",
      "|sentence                    |words                            |ngrams                                         |\n",
      "+----------------------------+---------------------------------+-----------------------------------------------+\n",
      "|Me encanta Spark            |[me, encanta, spark]             |[me encanta, encanta spark]                    |\n",
      "|Me encanta Python           |[me, encanta, python]            |[me encanta, encanta python]                   |\n",
      "|El aprendizaje es fascinante|[el, aprendizaje, es, fascinante]|[el aprendizaje, aprendizaje es, es fascinante]|\n",
      "+----------------------------+---------------------------------+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, NGram\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"NGramExample\").getOrCreate()\n",
    "\n",
    "# DataFrame de ejemplo con frases en español\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Me encanta Spark\"),\n",
    "    (0.0, \"Me encanta Python\"),\n",
    "    (1.0, \"El aprendizaje es fascinante\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "# Tokenizer para dividir las frases en palabras\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "# NGram para generar bigramas (n=2)\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "# Definir el pipeline con las etapas de tokenización y generación de bigramas\n",
    "pipeline = Pipeline(stages=[tokenizer, ngram])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = pipeline.fit(sentenceData)\n",
    "\n",
    "# Transformar los datos y mostrar el resultado\n",
    "model.transform(sentenceData).select(\"sentence\", \"words\", \"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be30cb74-f5a4-4ad5-bfae-12f5c2ef04fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Binarizer\n",
    "\n",
    "**Binarizer** es una herramienta en PySpark que convierte valores numéricos en valores binarios (0 o 1) en función de un umbral definido. Se utiliza en machine learning para transformar características continuas en características binarias, permitiendo que el modelo trate los datos en categorías simples de presencia o ausencia. \n",
    "\n",
    "#### Funcionamiento\n",
    "\n",
    "- **Umbral (`threshold`)**: `Binarizer` asigna el valor `1.0` a los datos que son iguales o superiores al umbral, y el valor `0.0` a los datos que son inferiores al umbral.\n",
    "- **Entrada y Salida**: Toma como entrada una columna numérica y genera una nueva columna con los valores binarizados.\n",
    "\n",
    "Por ejemplo, si aplicamos un umbral de `3.0` a la lista `[2.5, 3.5, 4.0, 2.0]`, el resultado será `[0, 1, 1, 0]`, ya que solo los valores mayores o iguales a `3.0` se convierten en `1.0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb8fecf-4fe9-418b-a7c5-543f5a577c0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.500000\n",
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    0.1|              0.0|\n",
      "|  1|    0.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "|  3|    0.5|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "# Crear DataFrame de ejemplo con valores continuos en la columna \"feature\"\n",
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 0.1),\n",
    "    (1, 0.8),\n",
    "    (2, 0.2),\n",
    "    (3, 0.5)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "# Configurar Binarizer para convertir \"feature\" a valores binarios en \"binarized_feature\" usando un umbral de 0.5\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "\n",
    "# Transformar el DataFrame\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "\n",
    "# Mostrar el umbral utilizado y el DataFrame resultante\n",
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "binarizedDataFrame.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2017d7da-6d3b-48d8-ba0d-50dbff4b02c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. StringIndexer\n",
    "\n",
    "**StringIndexer** es una herramienta en PySpark que convierte columnas categóricas con valores de texto en valores numéricos, asignando un índice único a cada categoría distinta. Esta transformación es esencial para el preprocesamiento de datos en machine learning, ya que muchos algoritmos requieren variables numéricas en lugar de texto.\n",
    "\n",
    "#### Funcionamiento\n",
    "\n",
    "1. **Asignación de Índices**: `StringIndexer` toma cada valor de texto en la columna de entrada y le asigna un índice numérico en función de la frecuencia de aparición:\n",
    "   - La categoría con mayor frecuencia recibe el índice `0`.\n",
    "   - La siguiente categoría más frecuente recibe el índice `1`, y así sucesivamente.\n",
    "\n",
    "2. **Columna de Salida**: La columna de salida contiene los índices correspondientes a cada valor de texto en la columna original, permitiendo que los algoritmos de machine learning trabajen con datos categóricos en formato numérico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd737ecd-41d5-4ba1-8e62-748777b38ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Crear DataFrame de ejemplo con columna categórica \"category\"\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a\"), \n",
    "    (1, \"b\"), \n",
    "    (2, \"c\"), \n",
    "    (3, \"a\"), \n",
    "    (4, \"a\"), \n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "# Configurar StringIndexer para transformar \"category\" a índices numéricos en \"categoryIndex\"\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Ajustar y transformar el DataFrame\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "indexed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c56e4010-a358-4cf5-b9aa-9074ada9e8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. LabelConverter\n",
    "\n",
    "**LabelConverter**, implementado en PySpark como `IndexToString`, es una herramienta utilizada para revertir la transformación de índices numéricos a etiquetas de texto originales en datos categóricos. Esto es útil en machine learning, especialmente cuando se han convertido etiquetas de texto a índices numéricos para entrenar el modelo, pero queremos interpretar y presentar los resultados finales en términos de sus categorías originales.\n",
    "\n",
    "#### Funcionamiento\n",
    "\n",
    "1. **Conversión Inicial con `StringIndexer`**:\n",
    "   - En muchos casos, antes de entrenar un modelo de machine learning, se convierte la columna de etiquetas de texto en índices numéricos con `StringIndexer`. Esto facilita que el modelo trabaje con datos categóricos.\n",
    "   \n",
    "2. **Reversión de Índices con `IndexToString`**:\n",
    "   - Una vez finalizado el entrenamiento y obtenidos los resultados (por ejemplo, predicciones), `IndexToString` permite revertir los índices numéricos de vuelta a las etiquetas de texto originales. Esto es esencial para hacer que los resultados sean interpretables para el usuario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03117ad3-e6e2-4df4-aa0d-b7cb06adcc75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column 'label' to indexed column 'labelIndex'\n",
      "+---+-----+----------+\n",
      "| id|label|labelIndex|\n",
      "+---+-----+----------+\n",
      "|  0|  Yes|       1.0|\n",
      "|  1|  Yes|       1.0|\n",
      "|  2|  Yes|       1.0|\n",
      "|  3|   No|       0.0|\n",
      "|  4|   No|       0.0|\n",
      "|  5|   No|       0.0|\n",
      "+---+-----+----------+\n",
      "\n",
      "Transformed indexed column 'labelIndex' back to original string column 'originalLabel' using labels in metadata\n",
      "+---+----------+-------------+\n",
      "| id|labelIndex|originalLabel|\n",
      "+---+----------+-------------+\n",
      "|  0|       1.0|          Yes|\n",
      "|  1|       1.0|          Yes|\n",
      "|  2|       1.0|          Yes|\n",
      "|  3|       0.0|           No|\n",
      "|  4|       0.0|           No|\n",
      "|  5|       0.0|           No|\n",
      "+---+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "# Crear un DataFrame de ejemplo con una columna de etiquetas de texto (\"Yes\" y \"No\")\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Yes\"), \n",
    "    (1, \"Yes\"), \n",
    "    (2, \"Yes\"), \n",
    "    (3, \"No\"), \n",
    "    (4, \"No\"), \n",
    "    (5, \"No\")\n",
    "], [\"id\", \"label\"])\n",
    "\n",
    "# Convertir la columna de etiquetas de texto a índices numéricos\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "print(\"Transformed string column '%s' to indexed column '%s'\" % \\\n",
    "    (indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.show()\n",
    "\n",
    "# Configurar IndexToString para revertir la columna de índices a los valores de texto originales\n",
    "converter = IndexToString(inputCol=\"labelIndex\", outputCol=\"originalLabel\")\n",
    "converted = converter.transform(indexed)\n",
    "\n",
    "# Mostrar el resultado de la conversión de vuelta a las etiquetas de texto originales\n",
    "print(\"Transformed indexed column '%s' back to original string column '%s' using \"\\\n",
    "    \"labels in metadata\" % (converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"labelIndex\", \"originalLabel\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76b56718-587d-4316-9be5-e29c4bfff74d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------------+\n",
      "| id|label|labelIndex|originalLabel|\n",
      "+---+-----+----------+-------------+\n",
      "|  0|  Yes|       1.0|          Yes|\n",
      "|  1|  Yes|       1.0|          Yes|\n",
      "|  2|  Yes|       1.0|          Yes|\n",
      "|  3|   No|       0.0|           No|\n",
      "|  4|   No|       0.0|           No|\n",
      "|  5|   No|       0.0|           No|\n",
      "+---+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer\n",
    "\n",
    "# Crear un DataFrame de ejemplo con una columna de etiquetas (\"Yes\" y \"No\")\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"Yes\"), \n",
    "    (1, \"Yes\"), \n",
    "    (2, \"Yes\"), \n",
    "    (3, \"No\"), \n",
    "    (4, \"No\"), \n",
    "    (5, \"No\")\n",
    "], [\"id\", \"label\"])\n",
    "\n",
    "# Configurar StringIndexer para convertir la columna de texto \"label\" en índices numéricos\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
    "\n",
    "# Configurar IndexToString para convertir los índices de vuelta a las etiquetas originales\n",
    "converter = IndexToString(inputCol=\"labelIndex\", outputCol=\"originalLabel\")\n",
    "\n",
    "# Crear un pipeline con las etapas de indexación y conversión\n",
    "pipeline = Pipeline(stages=[indexer, converter])\n",
    "\n",
    "# Ajustar el modelo y aplicar la transformación al DataFrame\n",
    "model = pipeline.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "# Mostrar el resultado\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65959f3f-2fff-4ae9-be58-10a1396fe6be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. VectorIndexer\n",
    "\n",
    "**VectorIndexer** es una herramienta en PySpark que ayuda a identificar y transformar automáticamente columnas categóricas en un vector de características. Este transformador es especialmente útil cuando se trabaja con datos en forma de vectores que contienen tanto características numéricas continuas como categóricas. Al indexar automáticamente las columnas categóricas en un vector, `VectorIndexer` facilita el preprocesamiento de datos para algoritmos de machine learning que requieren categorías representadas numéricamente.\n",
    "\n",
    "#### ¿Cómo Funciona?\n",
    "\n",
    "1. **Identificación Automática de Columnas Categóricas**:\n",
    "   - `VectorIndexer` analiza las características en el vector y considera una característica como categórica si tiene un número de valores distintos menor o igual a un umbral (`maxCategories`). Este parámetro se puede ajustar según el conjunto de datos.\n",
    "\n",
    "2. **Indexación de Columnas Categóricas**:\n",
    "   - Para cada columna categórica identificada, `VectorIndexer` asigna un índice numérico a cada categoría. Esto convierte valores categóricos en valores numéricos que los modelos de machine learning pueden procesar.\n",
    "\n",
    "3. **Compatibilidad con Algoritmos de Machine Learning**:\n",
    "   - La indexación de categorías es útil para algoritmos que no pueden trabajar directamente con datos categóricos, como los árboles de decisión y otros modelos basados en reglas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aba4ec0-bf0d-4e42-8d9c-b89046b5cde8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------+---------------------------------------+\n",
      "|label|features                               |indexedFeatures                        |\n",
      "+-----+---------------------------------------+---------------------------------------+\n",
      "|0    |(14,[0,1,3,12],[1.5,1.0,1.0,1.0])      |(14,[0,1,3,12],[1.5,1.0,1.0,1.0])      |\n",
      "|1    |(14,[0,5,7,10],[2.5,1.0,1.0,1.0])      |(14,[0,5,7,10],[2.5,1.0,1.0,1.0])      |\n",
      "|0    |(14,[0,1,4,6,11],[4.0,1.0,1.0,1.0,1.0])|(14,[0,1,4,6,11],[4.0,1.0,1.0,1.0,1.0])|\n",
      "|1    |(14,[0,2,8],[3.2,1.0,1.0])             |(14,[0,2,8],[3.2,1.0,1.0])             |\n",
      "|0    |(14,[0,1,9,13],[5.0,1.0,1.0,1.0])      |(14,[0,1,9,13],[5.0,1.0,1.0,1.0])      |\n",
      "+-----+---------------------------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorIndexer, RFormula\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"VectorIndexerPeruExample\").getOrCreate()\n",
    "\n",
    "# Crear un DataFrame de ejemplo con características de distintos tipos, usando departamentos de Perú\n",
    "df = spark.createDataFrame([\n",
    "    (0, 1.5, True, \"3\", \"turismo\", \"Lima\"),\n",
    "    (1, 2.5, False, \"5\", \"industria\", \"Arequipa\"),\n",
    "    (0, 4.0, True, \"4\", \"agricultura\", \"Cusco\"),\n",
    "    (1, 3.2, False, \"2\", \"minería\", \"Puno\"),\n",
    "    (0, 5.0, True, \"6\", \"pesca\", \"Piura\")\n",
    "], ['label', \"pib\", \"costa\", \"ranking\", \"sector\", \"departamento\"])\n",
    "\n",
    "# Configurar RFormula para generar automáticamente el vector de características\n",
    "formula = RFormula(\n",
    "    formula=\"label ~ pib + costa + ranking + sector + departamento\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\"\n",
    ")\n",
    "\n",
    "# Configurar VectorIndexer para identificar y convertir características categóricas\n",
    "# maxCategories=2 considera como categóricas solo las columnas con <= 2 valores distintos\n",
    "featureIndexer = VectorIndexer(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"indexedFeatures\",\n",
    "    maxCategories=2\n",
    ")\n",
    "\n",
    "# Crear un pipeline con RFormula y VectorIndexer\n",
    "pipeline = Pipeline(stages=[formula, featureIndexer])\n",
    "\n",
    "# Ajustar el pipeline y transformar el DataFrame\n",
    "model = pipeline.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "# Mostrar el DataFrame transformado\n",
    "result.select(\"label\", \"features\", \"indexedFeatures\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2bbcc8-b3b5-4f9e-b474-c224e652c8fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. VectorAssembler\n",
    "\n",
    "**VectorAssembler** es una herramienta en PySpark que combina múltiples columnas en un solo **vector de características**. Esto es útil en machine learning cuando se necesita consolidar varias características en un único vector para alimentar modelos de aprendizaje automático. **VectorAssembler** puede manejar tanto columnas numéricas como vectoriales.\n",
    "\n",
    "#### Funcionamiento\n",
    "\n",
    "1. **Agrupación de Columnas**:\n",
    "   - **VectorAssembler** toma varias columnas (numéricas o vectoriales) de un `DataFrame` y las concatena en una única columna de vector de salida, permitiendo que todas las características se representen en un solo vector.\n",
    "\n",
    "2. **Preparación para Modelos**:\n",
    "   - La mayoría de los modelos de PySpark requieren una columna de características unificada. **VectorAssembler** facilita esto y asegura la compatibilidad de los datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a518b94-5788-4360-8074-59f954d36f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas ensambladas 'duracionLlamada', 'tipoPlan', 'actividadUsuario' en la columna de vector 'caracteristicas'\n",
      "+-------------------------+-------------+\n",
      "|caracteristicas          |realizoCompra|\n",
      "+-------------------------+-------------+\n",
      "|[300.0,1.0,5.0,2.0,10.0] |1            |\n",
      "|[50.0,0.0,2.0,1.0,0.0]   |0            |\n",
      "|[600.0,1.0,15.0,5.0,25.0]|1            |\n",
      "+-------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Crear un DataFrame de ejemplo con características relacionadas con datos de telefonía\n",
    "dataset = spark.createDataFrame([\n",
    "    (0, 300, 1.0, Vectors.dense([5.0, 2.0, 10.0]), 1),  # 300 segundos de llamada, usuario en plan pospago, actividad de usuario\n",
    "    (1, 50, 0.0, Vectors.dense([2.0, 1.0, 0.0]), 0),    # 50 segundos de llamada, usuario en plan prepago, actividad de usuario\n",
    "    (2, 600, 1.0, Vectors.dense([15.0, 5.0, 25.0]), 1)  # 600 segundos de llamada, usuario en plan pospago, actividad de usuario\n",
    "], [\"id\", \"duracionLlamada\", \"tipoPlan\", \"actividadUsuario\", \"realizoCompra\"])\n",
    "\n",
    "# Configurar VectorAssembler para combinar las columnas en un vector de características\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"duracionLlamada\", \"tipoPlan\", \"actividadUsuario\"],\n",
    "    outputCol=\"caracteristicas\"\n",
    ")\n",
    "\n",
    "# Transformar el DataFrame\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Columnas ensambladas 'duracionLlamada', 'tipoPlan', 'actividadUsuario' en la columna de vector 'caracteristicas'\")\n",
    "output.select(\"caracteristicas\", \"realizoCompra\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c201db5f-09c4-473f-8c35-cb08a705ec99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9. OneHotEncoder\n",
    "\n",
    "**OneHotEncoder** es una técnica de codificación de variables categóricas que convierte cada categoría en un **vector binario**. Este vector tiene un valor de `1` en la posición de la categoría correspondiente y `0` en todas las demás posiciones. **OneHotEncoder** es ampliamente utilizado en machine learning, ya que muchos algoritmos requieren datos en formato numérico o binario.\n",
    "\n",
    "#### ¿Cómo Funciona?\n",
    "\n",
    "1. **Conversión de Categorías a Índices**:\n",
    "   - Para aplicar **OneHotEncoder**, primero es necesario convertir las categorías en índices numéricos utilizando `StringIndexer`. Cada categoría en una columna es asignada a un índice único.\n",
    "   \n",
    "2. **Codificación One-Hot**:\n",
    "   - Una vez convertidas las categorías a índices, **OneHotEncoder** genera un vector binario donde cada posición corresponde a una categoría. Si la categoría de una observación está presente en una columna, se le asigna `1` en su posición, y `0` en las demás.\n",
    "   \n",
    "3. **Salida en Formato Vectorial**:\n",
    "   - **OneHotEncoder** produce una columna de salida con un vector para cada observación, representando la presencia de una categoría específica de manera numérica y binaria. Esto permite que los datos categóricos sean compatibles con modelos de machine learning que no pueden trabajar directamente con datos de texto.\n",
    "\n",
    "#### Ejemplo Rápido de OneHotEncoder\n",
    "\n",
    "Supongamos que tenemos una columna de datos con tipos de frutas:\n",
    "\n",
    "- **Manzana**\n",
    "- **Naranja**\n",
    "- **Banana**\n",
    "\n",
    "Después de aplicar `StringIndexer`, los valores pueden quedar así:\n",
    "- **Manzana** → 0\n",
    "- **Naranja** → 1\n",
    "- **Banana** → 2\n",
    "\n",
    "Con **OneHotEncoder**, estos índices se transforman en vectores:\n",
    "- **Manzana** → `[1.0, 0.0, 0.0]`\n",
    "- **Naranja** → `[0.0, 1.0, 0.0]`\n",
    "- **Banana** → `[0.0, 0.0, 1.0]`\n",
    "\n",
    "### Visualización del Proceso\n",
    "\n",
    "1. **Datos Originales**:\n",
    "   | Fruta    |\n",
    "   |----------|\n",
    "   | Manzana  |\n",
    "   | Naranja  |\n",
    "   | Banana   |\n",
    "\n",
    "2. **Transformación con StringIndexer**:\n",
    "   | Fruta    | Índice |\n",
    "   |----------|--------|\n",
    "   | Manzana  | 0      |\n",
    "   | Naranja  | 1      |\n",
    "   | Banana   | 2      |\n",
    "\n",
    "3. **Codificación One-Hot**:\n",
    "   | Fruta    | Vector One-Hot        |\n",
    "   |----------|-----------------------|\n",
    "   | Manzana  | `[1.0, 0.0, 0.0]`     |\n",
    "   | Naranja  | `[0.0, 1.0, 0.0]`     |\n",
    "   | Banana   | `[0.0, 0.0, 1.0]`     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5ab3ed-e783-4160-a96e-1b0e6787a81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Import and creating SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8b30ad-b206-47be-93db-44ff738bc891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark create RDD example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3056053-db7b-4348-b407-7f739da63c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|       c|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "                            (0, \"a\"),\n",
    "                            (1, \"b\"),\n",
    "                            (2, \"c\"),\n",
    "                            (3, \"a\"),\n",
    "                            (4, \"a\"),\n",
    "                            (5, \"c\")\n",
    "                            ], [\"id\", \"category\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1f0a99-467b-48b5-bc84-b58b405226b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee36a68a-8885-47c6-b6e2-18d45a4712ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Codificación de Categoría a Índice Numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9454b050-1b2c-4855-b63b-5727ff82c805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|\n",
      "+---+--------+-------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|\n",
      "|  1|       b|          2.0|    (2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|\n",
      "+---+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Codificación de la categoría a índice numérico\n",
    "# Se utiliza StringIndexer para convertir la columna de categoría en índices numéricos\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(df)  # Ajustar el modelo a los datos\n",
    "indexed = model.transform(df)    # Transformar el DataFrame original\n",
    "\n",
    "# Codificación One-Hot, se debe ajustar y luego transformar\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoder_model = encoder.fit(indexed)  # Ajuste del modelo de codificación\n",
    "encoded = encoder_model.transform(indexed)  # Transformación de los datos\n",
    "\n",
    "# Mostrar el DataFrame codificado\n",
    "encoded.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3961a593-3289-47f5-83fb-ec1247705bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Pipeline para Codificación y Vectorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb8f197-6aeb-4858-8d38-06e0ee7e7e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Definir las columnas categóricas a indexar\n",
    "categoricalCols = ['category']\n",
    "\n",
    "# Crear una lista de indexadores para las columnas categóricas\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categoricalCols]\n",
    "\n",
    "# Crear una lista de OneHotEncoders para cada columna indexada\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol()), \n",
    "            dropLast=False) for indexer in indexers]\n",
    "\n",
    "# Usar VectorAssembler para combinar las columnas codificadas en un vector de características\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders], outputCol=\"features\")\n",
    "\n",
    "# Crear un Pipeline con los indexadores, encoders y el ensamblador\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "# Ajustar el modelo del pipeline a los datos\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Transformar los datos usando el modelo del pipeline\n",
    "data = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cedd586-356d-44ad-ab48-fc3e8f957d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Mostrar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9807c182-389a-4f7f-b442-586d83bfd329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------------+------------------------+-------------+\n",
      "| id|category|category_indexed|category_indexed_encoded|     features|\n",
      "+---+--------+----------------+------------------------+-------------+\n",
      "|  0|       a|             0.0|           (3,[0],[1.0])|[1.0,0.0,0.0]|\n",
      "|  1|       b|             2.0|           (3,[2],[1.0])|[0.0,0.0,1.0]|\n",
      "|  2|       c|             1.0|           (3,[1],[1.0])|[0.0,1.0,0.0]|\n",
      "|  3|       a|             0.0|           (3,[0],[1.0])|[1.0,0.0,0.0]|\n",
      "|  4|       a|             0.0|           (3,[0],[1.0])|[1.0,0.0,0.0]|\n",
      "|  5|       c|             1.0|           (3,[1],[1.0])|[0.0,1.0,0.0]|\n",
      "+---+--------+----------------+------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e31c1ea-5ae2-4b26-969e-34e1cb5adec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 10. Scale\n",
    "\n",
    "El **escalado de características** es una técnica crucial en el preprocesamiento de datos de machine learning. Ayuda a normalizar las características para mejorar el rendimiento y la precisión de los modelos. En PySpark, hay varias técnicas de escalado que se pueden aplicar:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c12486-b537-4687-8411-a9a5aaa87c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler,MaxAbsScaler\n",
    "scaler_type = 'Normal'\n",
    "if scaler_type=='Normal':\n",
    "    scaler = Normalizer(inputCol=\"features\", outputCol=\"scaledFeatures\", p=1.0)\n",
    "elif scaler_type=='Standard':\n",
    "    scaler = StandardScaler(inputCol=\"features\",\n",
    "                            outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "elif scaler_type=='MinMaxScaler':\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "elif scaler_type=='MaxAbsScaler':\n",
    "    scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f7e470-30f7-4dc6-b0d0-6a8ad568a3c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.5,-1.0]|\n",
      "|  1| [2.0,1.0,1.0]|\n",
      "|  2|[4.0,10.0,2.0]|\n",
      "+---+--------------+\n",
      "\n",
      "+---+--------------+------------------+\n",
      "| id|      features|    scaledFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "                            (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "                            (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "                            ], [\"id\", \"features\"])\n",
    "df.show()\n",
    "pipeline = Pipeline(stages=[scaler])\n",
    "model =pipeline.fit(df)\n",
    "data = model.transform(df)\n",
    "data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb07a569-9ff1-4eac-9660-39f6000ac856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Normalizer\n",
    "\n",
    "**Normalizer** es una técnica de preprocesamiento utilizada para escalar cada vector de características de modo que tenga una **norma (longitud) de 1**. Esto se realiza dividiendo cada valor del vector por la longitud total del vector, manteniendo las proporciones relativas entre las características. Esta técnica es particularmente útil cuando se quiere mantener la **dirección** del vector de características, pero no su magnitud.\n",
    "\n",
    "#### ¿Cómo Funciona?\n",
    "\n",
    "**Normalizer** ajusta los datos dividiendo cada valor del vector por su norma. La norma es una medida de la magnitud del vector. Normalmente, se utiliza la **norma Euclidiana (L2)** o la **norma Manhattan (L1)**.\n",
    "\n",
    "- **Fórmula General de Normalización**:\n",
    "  \n",
    "  `normalized_vector = vector / ||vector||`\n",
    "\n",
    "\n",
    "  Donde `||vector||` representa la norma del vector\n",
    "\n",
    "- **Norma \\(L^2\\) (Euclidiana)**:\n",
    "  $$ \n",
    "  \\|\\text{vector}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2} \n",
    "  $$\n",
    "  Esta norma se utiliza para garantizar que la longitud del vector sea 1.\n",
    "\n",
    "- **Norma \\(L^1\\) (Manhattan)**:\n",
    "  $$\n",
    "  \\|\\text{vector}\\|_1 = \\sum_{i=1}^n |x_i|\n",
    "  $$\n",
    "  Esta norma se usa cuando queremos normalizar usando la suma de los valores absolutos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd7af9e7-4cc1-4247-ba6c-e6c60df51f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizado usando la norma L1\n",
      "+---+--------------+------------------+\n",
      "| id|      features|      normFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n",
      "Normalizado usando la norma L∞\n",
      "+---+--------------+--------------+\n",
      "| id|      features|  normFeatures|\n",
      "+---+--------------+--------------+\n",
      "|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n",
      "|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n",
      "|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n",
      "+---+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Crear un DataFrame de ejemplo con vectores de características\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0])),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]))\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Normalizar cada vector usando la norma L1 (Manhattan)\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalizado usando la norma L1\")\n",
    "l1NormData.show()\n",
    "\n",
    "# Normalizar cada vector usando la norma L∞ (Máximo)\n",
    "lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n",
    "print(\"Normalizado usando la norma L∞\")\n",
    "lInfNormData.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87908708-04e8-4465-81e7-6289144a78c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### StandardScaler\n",
    "\n",
    "**StandardScaler** es una técnica de preprocesamiento utilizada para **escalar** y **centrar** las características de un dataset de modo que tengan una **media de 0** y una **desviación estándar de 1**. Esto se logra restando la media de cada característica y dividiendo por la desviación estándar. Es particularmente útil cuando se desea que todas las características tengan la misma importancia y escala.\n",
    "\n",
    "#### ¿Cómo Funciona?\n",
    "\n",
    "**StandardScaler** ajusta los datos haciendo que tengan una media de 0 y una desviación estándar de 1. Esta técnica ayuda a evitar que características con rangos de valores mayores dominen a otras durante el proceso de aprendizaje.\n",
    "\n",
    "- **Fórmula de Estandarización**:\n",
    "\n",
    "  `scaled_value = (x - μ) / σ`\n",
    "\n",
    "  Donde:\n",
    "  - `x` es el valor de la característica original.\n",
    "  - `μ` es la **media** de la característica.\n",
    "  - `σ` es la **desviación estándar** de la característica.\n",
    "\n",
    "- **Proceso**:\n",
    "  1. **Restar la Media**: Primero, se resta la media de cada característica, centrando los datos alrededor de 0.\n",
    "  2. **Dividir por la Desviación Estándar**: Luego, se divide cada característica por su desviación estándar para garantizar que todas tengan una varianza uniforme.\n",
    "\n",
    "#### Uso Común de StandardScaler\n",
    "\n",
    "- **Modelos Basados en Gradiente**:\n",
    "  - StandardScaler es útil para algoritmos como **regresión lineal** o **k-means**, que son sensibles a la escala de los datos. Ayuda a garantizar que cada característica tenga el mismo rango de influencia en el modelo.\n",
    "  \n",
    "- **PCA (Análisis de Componentes Principales)**:\n",
    "  - En técnicas como **PCA**, StandardScaler asegura que todas las características contribuyan de manera equitativa al análisis de la varianza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b11e4815-09a0-46e5-a6a9-64058244267f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------------------------------------+\n",
      "|features      |scaledFeatures                                                |\n",
      "+--------------+--------------------------------------------------------------+\n",
      "|[1.0,0.5,-1.0]|[-0.8728715609439696,-0.6234796863885498,-1.0910894511799618] |\n",
      "|[2.0,1.0,1.0] |[-0.21821789023599245,-0.5299577334302673,0.21821789023599242]|\n",
      "|[4.0,10.0,2.0]|[1.0910894511799618,1.1534374198188169,0.8728715609439697]    |\n",
      "+--------------+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"StandardScalerExample\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0])),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]))\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "\n",
    "# Crear el escalador\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withMean=True, withStd=True)\n",
    "\n",
    "# Ajustar el modelo del escalador a los datos\n",
    "scaler_model = scaler.fit(df)\n",
    "\n",
    "# Transformar los datos con el modelo ajustado\n",
    "scaled_data = scaler_model.transform(df)\n",
    "\n",
    "# Mostrar los resultados\n",
    "scaled_data.select(\"features\", \"scaledFeatures\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a29248-6c53-4c2f-9a60-f2e543f0eff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### MinMaxScaler\n",
    "\n",
    "##### ¿Cómo Funciona?\n",
    "\n",
    "**MinMaxScaler** transforma los datos al llevar cada característica dentro de un rango específico, por lo general entre 0 y 1.\n",
    "\n",
    "- **Fórmula de Escalado con MinMaxScaler**:\n",
    "\n",
    "  `scaled_value = (x - min) / (max - min)`\n",
    "\n",
    "  Donde:\n",
    "  - `x` es el valor original de la característica.\n",
    "  - `min` es el valor mínimo de la característica en el dataset.\n",
    "  - `max` es el valor máximo de la característica en el dataset.\n",
    "\n",
    "##### Propósito\n",
    "\n",
    "- **Uniformidad**: Garantiza que todas las características se encuentren dentro de un mismo rango de valores, lo cual puede ser muy beneficioso para algoritmos basados en distancia (como K-Means) y redes neuronales.\n",
    "- **Preservación de las Relaciones**: Al contrario de la estandarización, **MinMaxScaler** no cambia la distribución relativa de los datos, solo ajusta los valores para que se encuentren dentro del rango especificado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2555ce8f-8be9-4404-b934-b1cf7579b341",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------------------------+\n",
      "|id |features      |scaledFeatures                                             |\n",
      "+---+--------------+-----------------------------------------------------------+\n",
      "|0  |[1.0,0.5,-1.0]|(3,[],[])                                                  |\n",
      "|1  |[2.0,1.0,1.0] |[0.3333333333333333,0.05263157894736842,0.6666666666666666]|\n",
      "|2  |[4.0,10.0,2.0]|[1.0,1.0,1.0]                                              |\n",
      "+---+--------------+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Crear un DataFrame de ejemplo con vectores de características\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0])),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]))\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Crear MinMaxScaler y configurarlo para escalar los datos entre 0 y 1\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Ajustar el escalador a los datos y luego transformar los datos\n",
    "scaler_model = scaler.fit(dataFrame)\n",
    "scaled_data = scaler_model.transform(dataFrame)\n",
    "\n",
    "# Mostrar los datos escalados\n",
    "scaled_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c5cd15e-eb29-45b1-bc97-810e31b7d43d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 11. PCA\n",
    "\n",
    "**PCA (Principal Component Analysis)** es una técnica de reducción de dimensionalidad que se utiliza para **transformar** un conjunto de características posiblemente correlacionadas en un conjunto menor de **componentes principales**, que son combinaciones lineales de las características originales. Esto se logra de manera que se conserve la mayor cantidad de **varianza** posible de los datos originales.\n",
    "\n",
    "#### ¿Qué es PCA?\n",
    "\n",
    "El propósito principal de **PCA** es reducir la **dimensionalidad** del dataset para simplificar el análisis y la visualización, mientras se conserva la **información más relevante**. Los componentes principales se eligen de manera que sean **ortogonales** (sin correlación) entre sí y que cada componente represente la mayor cantidad de varianza posible.\n",
    "\n",
    "#### Cómo Funciona PCA\n",
    "\n",
    "- **Varianza**: PCA proyecta los datos en un nuevo espacio en el cual la varianza de los datos es máxima. \n",
    "- **Componentes Principales**:\n",
    "  - Cada componente principal es una combinación lineal de las características originales.\n",
    "  - El primer componente principal es la dirección que tiene la mayor varianza.\n",
    "  - El segundo componente principal es la dirección ortogonal al primer componente y con la siguiente mayor varianza, y así sucesivamente.\n",
    "\n",
    "#### Fórmula General\n",
    "\n",
    "- Supongamos que tenemos un conjunto de datos \\( X \\) con \\( n \\) características.\n",
    "- PCA transforma los datos mediante una combinación lineal:\n",
    "  \\[\n",
    "  Z = XW\n",
    "  \\]\n",
    "  Donde:\n",
    "  - \\( X \\) es la matriz original de datos.\n",
    "  - \\( W \\) es la matriz de pesos (autovectores).\n",
    "  - \\( Z \\) es la nueva matriz de componentes principales.\n",
    "\n",
    "#### Beneficios de PCA\n",
    "\n",
    "1. **Reducción de Dimensionalidad**:\n",
    "   - Ayuda a **simplificar** los datos, lo cual es particularmente útil cuando tenemos un gran número de características, y muchas de ellas están correlacionadas.\n",
    "  \n",
    "2. **Evita la Colinealidad**:\n",
    "   - Debido a que los componentes principales son **ortogonales** entre sí, **PCA** elimina la **colinealidad** entre las características.\n",
    "\n",
    "3. **Aumenta la Eficiencia del Modelo**:\n",
    "   - Al reducir el número de características, se reduce el **costo computacional**, lo cual mejora la eficiencia de los modelos de aprendizaje automático.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e664bc-410f-48c7-800a-5ecd9c7123e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------+\n",
      "|id |pcaFeatures                              |\n",
      "+---+-----------------------------------------+\n",
      "|0  |[-0.5112783832867785,-0.685424930259303] |\n",
      "|1  |[-1.6835922521434263,1.2832497998285195] |\n",
      "|2  |[-10.884223309499816,0.12204863447901526]|\n",
      "+---+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"PCAExample\").getOrCreate()\n",
    "\n",
    "# Crear un DataFrame con vectores de características\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0])),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]))\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Crear el PCA, reduciendo a 2 componentes principales\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "\n",
    "# Ajustar el modelo de PCA y transformar los datos\n",
    "pca_model = pca.fit(dataFrame)\n",
    "pca_result = pca_model.transform(dataFrame)\n",
    "\n",
    "# Mostrar los resultados\n",
    "pca_result.select(\"id\", \"pcaFeatures\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00d91e9-f464-4574-b65e-299679dee11d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 14. DCT\n",
    "\n",
    "**DCT (Discrete Cosine Transform)** o Transformada Discreta del Coseno es una técnica utilizada para **transformar datos en el dominio del tiempo** o **espacio** a un dominio de **frecuencias**. La DCT es ampliamente utilizada en **procesamiento de señales**, **compresión de imágenes**, y **machine learning** para capturar patrones que no son fácilmente visibles en los datos originales.\n",
    "\n",
    "#### ¿Qué es DCT?\n",
    "\n",
    "La **Transformada Discreta del Coseno** transforma un vector de características en un nuevo conjunto de valores que representan la amplitud de las frecuencias en los datos. La **DCT** convierte un **vector** de valores en una combinación de funciones coseno que representan las diferentes **frecuencias** presentes en el vector. Se suele utilizar cuando es necesario analizar los datos en términos de sus componentes de frecuencia, para simplificar el análisis o la compresión.\n",
    "\n",
    "#### Cómo Funciona DCT\n",
    "\n",
    "- **Transformación de Frecuencia**: La DCT toma un conjunto de datos en el dominio del tiempo o espacio y calcula una serie de **coeficientes** que representan las frecuencias que componen los datos.\n",
    "- **Componentes de Baja y Alta Frecuencia**:\n",
    "  - La DCT transforma los datos en componentes de **baja frecuencia** y **alta frecuencia**, lo cual es útil para analizar tendencias o patrones subyacentes.\n",
    "\n",
    "#### Fórmula de la Transformación DCT\n",
    "\n",
    "La fórmula de la **DCT tipo II** (que es la más común) para un vector de tamaño \\( N \\) es:\n",
    "\n",
    "$$\n",
    "X_k = \\sum_{n=0}^{N-1} x_n \\cos\\left(\\frac{\\pi k (2n + 1)}{2N}\\right)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- \\( x_n \\) son los valores originales del vector.\n",
    "- \\( X_k \\) es el coeficiente del coseno para la frecuencia \\( k \\).\n",
    "\n",
    "#### Beneficios de la DCT\n",
    "\n",
    "1. **Compresión de Información**:\n",
    "   - **DCT** es ampliamente utilizada en **compresión de imágenes** y **audio** (como JPEG y MP3) ya que permite retener la información más relevante en **pocos coeficientes**.\n",
    "\n",
    "2. **Reducción de Ruido**:\n",
    "   - Los **componentes de alta frecuencia** suelen representar ruido, por lo que al transformar los datos y quedarnos solo con los primeros coeficientes, se puede **reducir el ruido**.\n",
    "\n",
    "3. **Mejor Representación**:\n",
    "   - Al transformar los datos en el dominio de frecuencia, se pueden identificar **patrones** que no son evidentes en los datos originales.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc776781-9e4f-46de-8cd2-51103eb83bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------------------------+\n",
      "|id |dctFeatures                                                      |\n",
      "+---+-----------------------------------------------------------------+\n",
      "|0  |[5.0,-2.2304424973876635,0.0,-0.15851266778110737]               |\n",
      "|1  |[5.0,2.2304424973876635,0.0,0.15851266778110737]                 |\n",
      "|2  |[-1.0,1.8477590650225737,-1.0000000000000002,-0.7653668647301795]|\n",
      "+---+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import DCT\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"DCTExample\").getOrCreate()\n",
    "\n",
    "# Crear un DataFrame con vectores de características\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 2.0, 3.0, 4.0])),\n",
    "    (1, Vectors.dense([4.0, 3.0, 2.0, 1.0])),\n",
    "    (2, Vectors.dense([0.0, 1.0, -1.0, -2.0]))\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Crear la transformación DCT\n",
    "dct = DCT(inverse=False, inputCol=\"features\", outputCol=\"dctFeatures\")\n",
    "\n",
    "# Transformar los datos con DCT\n",
    "dct_data = dct.transform(dataFrame)\n",
    "\n",
    "# Mostrar los resultados\n",
    "dct_data.select(\"id\", \"dctFeatures\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3569f1d-fce1-4724-8362-e9364c129a95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2989fd58-4785-41eb-bb98-b78b2f60a4f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Feature Selection** o **Selección de Características** es una técnica clave en el preprocesamiento de datos para **reducir el número de características** que se utilizarán en un modelo de machine learning. El objetivo es seleccionar solo las características más relevantes que maximicen el rendimiento del modelo, reduciendo el **ruido** y evitando el **sobreajuste (overfitting)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3319783c-6529-4857-9831-5e055e53d0bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. LASSO\n",
    "\n",
    "**LASSO** es un tipo de regresión lineal que añade un término de penalización basado en la **suma de los valores absolutos de los coeficientes**. La penalización L1 es una forma de regularización que tiende a reducir el valor de los coeficientes. Al aumentar la fuerza de la penalización, algunos coeficientes se reducen a exactamente **cero**, y esto se utiliza para identificar y eliminar características irrelevantes.\n",
    "\n",
    "- **Penalización L1**:\n",
    "  - El término de penalización es proporcional a la **suma de los valores absolutos** de los coeficientes, es decir:\n",
    "  $$\n",
    "  \\text{Lasso Loss} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "  $$\n",
    "  Donde:\n",
    "  - `RSS` es el **Residual Sum of Squares** (suma de los cuadrados de los residuos).\n",
    "  - `λ` es el **parámetro de regularización** que controla la fuerza de la penalización.\n",
    "  - `β_j` representa los coeficientes del modelo.\n",
    "\n",
    "#### ¿Por Qué Usar LASSO para Selección de Características?\n",
    "\n",
    "1. **Eliminar Características Irrelevantes**:\n",
    "   - LASSO ajusta algunos coeficientes a cero, eliminando efectivamente las características que no contribuyen significativamente al modelo.\n",
    "\n",
    "2. **Reducir el Sobreajuste**:\n",
    "   - Al eliminar características innecesarias y reducir los coeficientes, LASSO reduce la complejidad del modelo, lo cual ayuda a prevenir el **sobreajuste**.\n",
    "\n",
    "3. **Mejora de la Interpretabilidad**:\n",
    "   - Con menos características en el modelo, es más fácil **interpretar** y **entender** el impacto de cada característica en la predicción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eee0e76-fffc-4595-92ab-177e498114d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Original:\n",
      "+------------+-----------+------------+-----------+---------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  species|\n",
      "+------------+-----------+------------+-----------+---------+\n",
      "|         5.1|        3.5|         1.4|        0.2|   setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|   setosa|\n",
      "|         6.2|        3.4|         5.4|        2.3|virginica|\n",
      "|         5.9|        3.0|         5.1|        1.8|virginica|\n",
      "|         5.4|        3.4|         1.7|        0.2|   setosa|\n",
      "+------------+-----------+------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Coeficientes: [0.0,0.0,0.0,0.8797878708974681]\n",
      "Intercepto: -0.0854870045330018\n",
      "Predicciones:\n",
      "+-----------------+-----+-------------------+\n",
      "|         features|label|         prediction|\n",
      "+-----------------+-----+-------------------+\n",
      "|[5.1,3.5,1.4,0.2]|  0.0|0.09047056964649185|\n",
      "|[4.9,3.0,1.4,0.2]|  0.0|0.09047056964649185|\n",
      "|[6.2,3.4,5.4,2.3]|  2.0| 1.9380250985311747|\n",
      "|[5.9,3.0,5.1,1.8]|  2.0| 1.4981311630824408|\n",
      "|[5.4,3.4,1.7,0.2]|  0.0|0.09047056964649185|\n",
      "|[6.5,2.8,4.6,1.5]|  1.0| 1.2341948018132003|\n",
      "|[5.7,2.8,4.5,1.3]|  1.0| 1.0582372276337069|\n",
      "+-----------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa las bibliotecas necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"LASSO_Iris_Regression\").getOrCreate()\n",
    "\n",
    "# Creación de un DataFrame simulado con el dataset Iris\n",
    "data = [\n",
    "    (5.1, 3.5, 1.4, 0.2, \"setosa\"),\n",
    "    (4.9, 3.0, 1.4, 0.2, \"setosa\"),\n",
    "    (6.2, 3.4, 5.4, 2.3, \"virginica\"),\n",
    "    (5.9, 3.0, 5.1, 1.8, \"virginica\"),\n",
    "    (5.4, 3.4, 1.7, 0.2, \"setosa\"),\n",
    "    (6.5, 2.8, 4.6, 1.5, \"versicolor\"),\n",
    "    (5.7, 2.8, 4.5, 1.3, \"versicolor\"),\n",
    "    # (Agrega más filas del dataset Iris completo si es necesario)\n",
    "]\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\n",
    "\n",
    "# Crea el DataFrame de Spark con los datos de Iris\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Muestra una vista previa del DataFrame original\n",
    "print(\"DataFrame Original:\")\n",
    "df.show(5)\n",
    "\n",
    "# Convertir la columna de clase \"species\" en un índice numérico para usar en el modelo de regresión\n",
    "indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "indexed_df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Combina las características en un solo vector llamado \"features\"\n",
    "assembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
    "assembled_df = assembler.transform(indexed_df)\n",
    "\n",
    "# Define el modelo de regresión lineal con regularización LASSO (L1 > 0)\n",
    "lasso = LinearRegression(featuresCol=\"features\", labelCol=\"label\", regParam=0.1, elasticNetParam=1.0)\n",
    "\n",
    "# Entrena el modelo\n",
    "lasso_model = lasso.fit(assembled_df)\n",
    "\n",
    "# Imprime los coeficientes para ver cuáles se han reducido a cero\n",
    "print(\"Coeficientes:\", lasso_model.coefficients)\n",
    "print(\"Intercepto:\", lasso_model.intercept)\n",
    "\n",
    "# Opcional: Muestra las predicciones para los datos de entrada\n",
    "predictions = lasso_model.transform(assembled_df)\n",
    "print(\"Predicciones:\")\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7aab5c77-2362-4ee3-be0f-dad2402bc7ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. RandomForestClassifier\n",
    "\n",
    "\n",
    "**Random Forest** es un conjunto de **árboles de decisión** entrenados de manera independiente. La predicción final se realiza mediante la combinación (votación) de las predicciones individuales de cada árbol. Esta técnica se destaca por:\n",
    "\n",
    "- Ser **robusta frente al sobreajuste** debido al promedio de las predicciones de muchos árboles.\n",
    "- Ser **flexible**, capaz de manejar características categóricas, numéricas, y grandes volúmenes de datos.\n",
    "\n",
    "#### Selección de Características con Random Forest\n",
    "\n",
    "**Random Forest** también sirve para la **selección de características**, ya que puede medir la **importancia** de cada característica basándose en cómo contribuye a las decisiones de los árboles durante la clasificación. Las características más importantes serán aquellas que más veces se utilicen para dividir nodos y que mejor separen las clases.\n",
    "\n",
    "- **Importancia de Característica**: Cada característica se mide según su contribución para reducir el **índice de impureza** en los nodos de los árboles.\n",
    "- **Índice de Gini**: En los árboles de decisión, el **índice de Gini** se utiliza para medir la impureza de una partición, es decir, cuán mezcladas están las clases en un nodo.\n",
    "\n",
    "La **importancia de una característica** en Random Forest se calcula basándose en la cantidad que contribuye cada característica para reducir el índice de Gini en las divisiones de los árboles. A mayor reducción de impureza, mayor es la importancia de la característica.\n",
    "\n",
    "#### Fórmula del Índice de Gini\n",
    "\n",
    "El **índice de Gini** para una partición se calcula como:\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- `C` es el número de clases.\n",
    "- `p_i` es la proporción de observaciones de la clase `i` en el nodo.\n",
    "\n",
    "Un valor de **Gini** cercano a 0 indica baja impureza (es decir, un nodo con observaciones de la misma clase), mientras que un valor cercano a 1 indica alta impureza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0f0968-d16c-494f-915e-36f19e4b8bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Original:\n",
      "+------------+-----------+------------+-----------+---------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  species|\n",
      "+------------+-----------+------------+-----------+---------+\n",
      "|         5.1|        3.5|         1.4|        0.2|   setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|   setosa|\n",
      "|         6.2|        3.4|         5.4|        2.3|virginica|\n",
      "|         5.9|        3.0|         5.1|        1.8|virginica|\n",
      "|         5.4|        3.4|         1.7|        0.2|   setosa|\n",
      "+------------+-----------+------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Predicciones:\n",
      "+-----------------+-----+----------+\n",
      "|         features|label|prediction|\n",
      "+-----------------+-----+----------+\n",
      "|[5.1,3.5,1.4,0.2]|  0.0|       0.0|\n",
      "|[4.9,3.0,1.4,0.2]|  0.0|       0.0|\n",
      "|[6.2,3.4,5.4,2.3]|  2.0|       2.0|\n",
      "|[5.9,3.0,5.1,1.8]|  2.0|       2.0|\n",
      "|[5.4,3.4,1.7,0.2]|  0.0|       0.0|\n",
      "+-----------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Precisión del modelo: 1.0\n",
      "Importancia de las características: (4,[0,1,2,3],[0.4025168858475074,0.06029938710550564,0.24769249484360928,0.28949123220337764])\n"
     ]
    }
   ],
   "source": [
    "# Importa las bibliotecas necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Inicializa la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"RandomForest_Iris_Classification\").getOrCreate()\n",
    "\n",
    "# Creación de un DataFrame simulado con el dataset Iris\n",
    "data = [\n",
    "    (5.1, 3.5, 1.4, 0.2, \"setosa\"),\n",
    "    (4.9, 3.0, 1.4, 0.2, \"setosa\"),\n",
    "    (6.2, 3.4, 5.4, 2.3, \"virginica\"),\n",
    "    (5.9, 3.0, 5.1, 1.8, \"virginica\"),\n",
    "    (5.4, 3.4, 1.7, 0.2, \"setosa\"),\n",
    "    (6.5, 2.8, 4.6, 1.5, \"versicolor\"),\n",
    "    (5.7, 2.8, 4.5, 1.3, \"versicolor\"),\n",
    "    # (Agrega más filas del dataset Iris completo si es necesario)\n",
    "]\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\n",
    "\n",
    "# Crea el DataFrame de Spark con los datos de Iris\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Muestra una vista previa del DataFrame original\n",
    "print(\"DataFrame Original:\")\n",
    "df.show(5)\n",
    "\n",
    "# Convertir la columna de clase \"species\" en un índice numérico\n",
    "indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "indexed_df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Combina las características en un solo vector llamado \"features\"\n",
    "assembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
    "assembled_df = assembler.transform(indexed_df)\n",
    "\n",
    "# Define el modelo de clasificación de Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, maxDepth=5)\n",
    "\n",
    "# Entrena el modelo de Random Forest\n",
    "rf_model = rf.fit(assembled_df)\n",
    "\n",
    "# Realiza predicciones en el mismo conjunto de datos\n",
    "predictions = rf_model.transform(assembled_df)\n",
    "\n",
    "# Mostrar las predicciones\n",
    "print(\"Predicciones:\")\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(5)\n",
    "\n",
    "# Evaluar el modelo usando la precisión\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Precisión del modelo: {accuracy}\")\n",
    "\n",
    "# Importancia de las características\n",
    "print(\"Importancia de las características:\", rf_model.featureImportances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12455fd8-9122-423c-a818-2f1ee7ef5377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Chi-Square\n",
    "\n",
    "\n",
    "La **prueba Chi-Cuadrado** se utiliza para determinar si existe una relación significativa entre cada característica y la **etiqueta de salida**. Se calcula un valor `χ^2` que indica cuán dependientes son las características respecto a la etiqueta. Las características con valores `χ^2` más altos son las más importantes y, por lo tanto, se seleccionan para formar el conjunto final.\n",
    "\n",
    "- **Valor Chi-Cuadrado (`χ^2`)**:\n",
    "  - Este valor mide cuán esperados o inesperados son los datos en comparación con una hipótesis nula.\n",
    "  - Un valor alto indica que existe una relación fuerte entre la característica y la variable de salida.\n",
    "\n",
    "##### Fórmula del Test Chi-Cuadrado\n",
    "\n",
    "La fórmula para calcular el valor `χ^2` es:\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- `O_i` es el valor **observado**.\n",
    "- `E_i` es el valor **esperado** bajo la hipótesis nula (que asume que no hay relación entre las variables).\n",
    "\n",
    "El valor calculado de `χ^2` se compara con un valor crítico basado en el nivel de significancia elegido y los **grados de libertad** del problema, para decidir si una característica tiene una relación significativa con la etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8a38ac-2404-4131-a482-9472d1017890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|features      |selectedFeatures|\n",
      "+--------------+----------------+\n",
      "|[0.0,30.0,2.0]|[30.0,2.0]      |\n",
      "|[1.0,25.0,1.0]|[25.0,1.0]      |\n",
      "|[2.0,40.0,2.0]|[40.0,2.0]      |\n",
      "|[0.0,20.0,1.0]|[20.0,1.0]      |\n",
      "|[1.0,50.0,0.0]|[50.0,0.0]      |\n",
      "|[2.0,15.0,0.0]|[15.0,0.0]      |\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import ChiSqSelector, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"ChiSquareFeatureSelectionExample\").getOrCreate()\n",
    "\n",
    "# Crear un DataFrame con características de empleados\n",
    "data = [\n",
    "    (1, \"Licenciatura\", 30, \"Ventas\", 1.0),\n",
    "    (0, \"Maestría\", 25, \"IT\", 0.0),\n",
    "    (1, \"Secundaria\", 40, \"Ventas\", 1.0),\n",
    "    (0, \"Licenciatura\", 20, \"IT\", 0.0),\n",
    "    (1, \"Maestría\", 50, \"Finanzas\", 1.0),\n",
    "    (0, \"Secundaria\", 15, \"Finanzas\", 0.0)\n",
    "]\n",
    "columns = [\"id\", \"educacion\", \"horas_capacitacion\", \"departamento\", \"promocion\"]\n",
    "\n",
    "# Crear un DataFrame con los datos de ejemplo\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convertir columnas categóricas a índices numéricos usando StringIndexer\n",
    "indexer_educacion = StringIndexer(inputCol=\"educacion\", outputCol=\"educacion_index\")\n",
    "indexer_departamento = StringIndexer(inputCol=\"departamento\", outputCol=\"departamento_index\")\n",
    "df_indexed = indexer_educacion.fit(df).transform(df)\n",
    "df_indexed = indexer_departamento.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "# Combinar las características en un vector con VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"educacion_index\", \"horas_capacitacion\", \"departamento_index\"], outputCol=\"features\")\n",
    "assembled_df = assembler.transform(df_indexed)\n",
    "\n",
    "# Aplicar ChiSqSelector para seleccionar las características más importantes\n",
    "selector = ChiSqSelector(numTopFeatures=2, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"promocion\")\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "result = selector.fit(assembled_df).transform(assembled_df)\n",
    "\n",
    "# Mostrar los resultados de las características seleccionadas\n",
    "result.select(\"features\", \"selectedFeatures\").show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Taller 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
